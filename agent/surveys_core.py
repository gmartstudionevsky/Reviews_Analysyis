# agent/surveys_core.py
# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è "—Å—ã—Ä—ã—Ö" –∞–Ω–∫–µ—Ç TL: Marketing –≤ –µ–¥–∏–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É.
# –ö–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ = –æ–¥–Ω–∞ –∞–Ω–∫–µ—Ç–∞ –≥–æ—Å—Ç—è.

from __future__ import annotations
import re, hashlib
from datetime import date
from typing import Dict, List, Optional
import pandas as pd
import numpy as np


# ==== –ú–∞–ø–ø–∏–Ω–≥ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–ª–æ–Ω–æ–∫ –∏–∑ –≤—ã–≥—Ä—É–∑–∫–∏ -> –Ω–∞—à–∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –∫–ª—é—á–∏ ====
# –ú—ã –ø—Ä–∏–≤–æ–¥–∏–º –∏—Ö –∫ –∫–æ—Ä–æ—Ç–∫–∏–º –º–∞—à–∏–Ω–æ—á–∏—Ç–∞–µ–º—ã–º –∏–º–µ–Ω–∞–º.
# –§–æ—Ä–º–∞—Ç: –Ω–∞—à_–∫–ª—é—á : [–≤–æ–∑–º–æ–∂–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –≤ –≤—ã–≥—Ä—É–∑–∫–µ]
COLUMN_ALIASES: Dict[str, List[str]] = {
    # —Å–ª—É–∂–µ–±–Ω—ã–µ
    "fio": [
        "–§–ò–û", "–ò–º—è", "–ò–º—è –≥–æ—Å—Ç—è", "–§.–ò.–û", "–§–∞–º–∏–ª–∏—è –ò–º—è",
    ],
    "booking": [
        "–ù–æ–º–µ—Ä –±—Ä–æ–Ω–∏", "–ë—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ", "–ë—Ä–æ–Ω—å", "–ù–æ–º–µ—Ä –∑–∞–∫–∞–∑–∞", "–ù–æ–º–µ—Ä —Ä–µ–∑–µ—Ä–≤–∞—Ü–∏–∏",
    ],
    "phone": ["–¢–µ–ª–µ—Ñ–æ–Ω", "–¢–µ–ª.", "–ù–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞", "–ö–æ–Ω—Ç–∞–∫—Ç–Ω—ã–π –Ω–æ–º–µ—Ä"],
    "email": ["Email", "E-mail", "–ü–æ—á—Ç–∞", "–ê–¥—Ä–µ—Å —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π –ø–æ—á—Ç—ã"],
    "comment": ["–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –≥–æ—Å—Ç—è", "–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π", "–û—Ç–∑—ã–≤", "–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –∫–ª–∏–µ–Ω—Ç–∞", "–í–∞—à–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏"],
    "survey_date": [
        "–î–∞—Ç–∞ –∞–Ω–∫–µ—Ç–∏—Ä–æ–≤–∞–Ω–∏—è", "–î–∞—Ç–∞ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è –æ–ø—Ä–æ—Å–∞", "–î–∞—Ç–∞ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è",
        "–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è", "–î–∞—Ç–∞ –æ–ø—Ä–æ—Å–∞", "–î–∞—Ç–∞ –∞–Ω–∫–µ—Ç—ã", "–î–∞—Ç–∞", "–î–∞—Ç–∞ –æ–±—Ä–∞—â–µ–Ω–∏—è",
    ],

    # –æ–±—â–µ–µ –≤–ø–µ—á–∞—Ç–ª–µ–Ω–∏–µ
    "overall": [
        "–°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –≥–æ—Å—Ç—è", "–ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞", "–û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞", "–û–±—â–∞—è —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–Ω–æ—Å—Ç—å",
    ],

    # –±–ª–æ–∫ 1: –∑–∞–µ–∑–¥ / –∑–∞—Å–µ–ª–µ–Ω–∏–µ
    "fo_checkin": [
        "‚Ññ 1.1 –û—Ü–µ–Ω–∏—Ç–µ —Ä–∞–±–æ—Ç—É —Å–ª—É–∂–±—ã –ø—Ä–∏—ë–º–∞ –∏ —Ä–∞–∑–º–µ—â–µ–Ω–∏—è –ø—Ä–∏ –∑–∞–µ–∑–¥–µ",
        "1.1 –ø—Ä–∏–µ–º –∏ —Ä–∞–∑–º–µ—â–µ–Ω–∏–µ –ø—Ä–∏ –∑–∞–µ–∑–¥–µ",
        "–û—Ü–µ–Ω–∏—Ç–µ —Ä–∞–±–æ—Ç—É —Å–ª—É–∂–±—ã –ø—Ä–∏—ë–º–∞ –∏ —Ä–∞–∑–º–µ—â–µ–Ω–∏—è –ø—Ä–∏ –∑–∞–µ–∑–¥–µ",
        "–°–ª—É–∂–±–∞ –ø—Ä–∏—ë–º–∞ –ø—Ä–∏ –∑–∞—Å–µ–ª–µ–Ω–∏–∏",
    ],
    "clean_checkin": [
        "‚Ññ 1.2 –û—Ü–µ–Ω–∏—Ç–µ —á–∏—Å—Ç–æ—Ç—É –Ω–æ–º–µ—Ä–∞ –ø—Ä–∏ –∑–∞–µ–∑–¥–µ",
        "1.2 —á–∏—Å—Ç–æ—Ç–∞ –ø—Ä–∏ –∑–∞–µ–∑–¥–µ",
        "–ß–∏—Å—Ç–æ—Ç–∞ –Ω–æ–º–µ—Ä–∞ –ø—Ä–∏ –∑–∞–µ–∑–¥–µ",
    ],
    "room_comfort": [
        "‚Ññ 1.3 –û—Ü–µ–Ω–∏—Ç–µ –∫–æ–º—Ñ–æ—Ä—Ç –∏ –æ—Å–Ω–∞—â–µ–Ω–∏–µ –Ω–æ–º–µ—Ä–∞",
        "1.3 –∫–æ–º—Ñ–æ—Ä—Ç –∏ –æ—Å–Ω–∞—â–µ–Ω–∏–µ –Ω–æ–º–µ—Ä–∞",
        "–ö–æ–º—Ñ–æ—Ä—Ç –∏ –æ—Å–Ω–∞—â–µ–Ω–∏–µ –Ω–æ–º–µ—Ä–∞",
        "–ö–æ–º—Ñ–æ—Ä—Ç –Ω–æ–º–µ—Ä–∞",
    ],

    # –±–ª–æ–∫ 2: –ø—Ä–æ–∂–∏–≤–∞–Ω–∏–µ
    "fo_stay": [
        "‚Ññ 2.1 –û—Ü–µ–Ω–∏—Ç–µ —Ä–∞–±–æ—Ç—É —Å–ª—É–∂–±—ã –ø—Ä–∏—ë–º–∞ –∏ —Ä–∞–∑–º–µ—â–µ–Ω–∏—è –≤–æ –≤—Ä–µ–º—è –ø—Ä–æ–∂–∏–≤–∞–Ω–∏—è",
        "2.1 –ø—Ä–∏–µ–º –∏ —Ä–∞–∑–º–µ—â–µ–Ω–∏–µ –≤–æ –≤—Ä–µ–º—è –ø—Ä–æ–∂–∏–≤–∞–Ω–∏—è",
        "–°–ª—É–∂–±–∞ –ø—Ä–∏—ë–º–∞ –≤–æ –≤—Ä–µ–º—è –ø—Ä–æ–∂–∏–≤–∞–Ω–∏—è",
        "–°–ª—É–∂–±–∞ —Ä–∞–∑–º–µ—â–µ–Ω–∏—è (–ø—Ä–æ–∂–∏–≤–∞–Ω–∏–µ)",
    ],
    "its_service": [
        "‚Ññ 2.2 –û—Ü–µ–Ω–∏—Ç–µ —Ä–∞–±–æ—Ç—É —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π —Å–ª—É–∂–±—ã",
        "2.2 —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Å–ª—É–∂–±–∞",
        "–†–∞–±–æ—Ç–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π —Å–ª—É–∂–±—ã",
        "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Å–ª—É–∂–±–∞",
    ],
    "hsk_stay": [
        "‚Ññ 2.3 –û—Ü–µ–Ω–∏—Ç–µ —É–±–æ—Ä–∫—É –Ω–æ–º–µ—Ä–∞ –≤–æ –≤—Ä–µ–º—è –ø—Ä–æ–∂–∏–≤–∞–Ω–∏—è",
        "2.3 —É–±–æ—Ä–∫–∞ –≤–æ –≤—Ä–µ–º—è –ø—Ä–æ–∂–∏–≤–∞–Ω–∏—è",
        "–£–±–æ—Ä–∫–∞ –Ω–æ–º–µ—Ä–∞ –≤–æ –≤—Ä–µ–º—è –ø—Ä–æ–∂–∏–≤–∞–Ω–∏—è",
        "Housekeeping –≤–æ –≤—Ä–µ–º—è –ø—Ä–æ–∂–∏–≤–∞–Ω–∏—è",
    ],
    "breakfast": [
        "‚Ññ 2.4 –û—Ü–µ–Ω–∏—Ç–µ –∑–∞–≤—Ç—Ä–∞–∫–∏",
        "2.4 –∑–∞–≤—Ç—Ä–∞–∫–∏",
        "–ó–∞–≤—Ç—Ä–∞–∫–∏",
        "–ö–∞—á–µ—Å—Ç–≤–æ –∑–∞–≤—Ç—Ä–∞–∫–∞",
    ],

    # –±–ª–æ–∫ 3: –æ—Ç–µ–ª—å –≤ —Ü–µ–ª–æ–º
    "atmosphere": [
        "‚Ññ 3.1 –û—Ü–µ–Ω–∏—Ç–µ –∞—Ç–º–æ—Å—Ñ–µ—Ä—É –≤ –æ—Ç–µ–ª–µ",
        "3.1 –∞—Ç–º–æ—Å—Ñ–µ—Ä–∞",
        "–ê—Ç–º–æ—Å—Ñ–µ—Ä–∞ –≤ –æ—Ç–µ–ª–µ",
    ],
    "location": [
        "‚Ññ 3.2 –û—Ü–µ–Ω–∏—Ç–µ —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ –æ—Ç–µ–ª—è",
        "3.2 —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ",
        "–†–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ –æ—Ç–µ–ª—è",
        "–õ–æ–∫–∞—Ü–∏—è –æ—Ç–µ–ª—è",
    ],
    "value": [
        "‚Ññ 3.3 –û—Ü–µ–Ω–∏—Ç–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Ü–µ–Ω—ã –∏ –∫–∞—á–µ—Å—Ç–≤–∞",
        "3.3 —Ü–µ–Ω–∞/–∫–∞—á–µ—Å—Ç–≤–æ",
        "–°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Ü–µ–Ω—ã –∏ –∫–∞—á–µ—Å—Ç–≤–∞",
        "–¶–µ–Ω–∞ / –∫–∞—á–µ—Å—Ç–≤–æ",
    ],
    "would_return": [
        "‚Ññ 3.4 –•–æ—Ç–µ–ª–∏ –±—ã –≤—ã –≤–µ—Ä–Ω—É—Ç—å—Å—è –≤ ARTSTUDIO Nevsky?",
        "3.4 –≤–µ—Ä–Ω—É–ª–∏—Å—å –±—ã",
        "–ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –≤–µ—Ä–Ω—É—Ç—å—Å—è",
        "–í–µ—Ä–Ω—É–ª–∏—Å—å –±—ã –∫ –Ω–∞–º —Å–Ω–æ–≤–∞",
    ],

    # NPS-–≤–æ–ø—Ä–æ—Å
    "nps5": [
        "‚Ññ 3.5 –û—Ü–µ–Ω–∏—Ç–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ç–æ–≥–æ, —á—Ç–æ –≤—ã –ø–æ—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç–µ –Ω–∞—Å –¥—Ä—É–∑—å—è–º –∏ –±–ª–∏–∑–∫–∏–º (–ø–æ —à–∫–∞–ª–µ –æ—Ç 1 –¥–æ 5)",
        "3.5 nps 1-5",
        "nps (1-5)",
        "nps 1-5",
        "–ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å (1-5)",
    ],
}


# ===== –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–º—ë–Ω —Å—Ç–æ–ª–±—Ü–æ–≤ =====
def _norm_name_for_match(s: str) -> str:
    """–û–ø—É—Å–∫–∞–µ–º —Ä–µ–≥–∏—Å—Ç—Ä, —É–±–∏—Ä–∞–µ–º nbsp, –ª–∏—à–Ω—é—é –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ –¥—É–±–ª–∏ –ø—Ä–æ–±–µ–ª–æ–≤."""
    t = str(s).replace("\u00a0", " ").strip().lower()
    t = re.sub(r"\s+", " ", t)
    t = re.sub(r"[^\w\s–∞-—è—ë0-9]", " ", t)
    t = re.sub(r"\s+", " ", t)
    return t.strip()

def _find_col(df: pd.DataFrame, aliases: List[str]) -> Optional[str]:
    """
    –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Ä–µ–∞–ª—å–Ω–æ–µ –∏–º—è –∫–æ–ª–æ–Ω–∫–∏ –≤ df, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–µ –æ–¥–Ω–æ–º—É –∏–∑ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ aliases.
    –î–µ–ª–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ—Ö–æ–¥–æ–≤: —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏, –ø–æ–¥—Å—Ç—Ä–æ–∫–∞ –∏ —Ç.–¥.
    """
    lowmap = {_norm_name_for_match(c): c for c in df.columns}

    # —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (–ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏)
    for cand in aliases:
        key = _norm_name_for_match(cand)
        if key in lowmap:
            return lowmap[key]

    # –ø–æ–¥—Å—Ç—Ä–æ–∫–∞
    for cand in aliases:
        key = _norm_name_for_match(cand)
        for lk, orig in lowmap.items():
            if key and key in lk:
                return orig

    # –≤—Å–µ —Å–ª–æ–≤–∞ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è
    for cand in aliases:
        words = [w for w in _norm_name_for_match(cand).split() if len(w) > 1]
        for lk, orig in lowmap.items():
            if all(w in lk for w in words):
                return orig

    return None


# ===== –ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–¥–µ–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –≤ —à–∫–∞–ª—É /5 =====
def _to_5_scale(val) -> float:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ –ø—Ä–∏–≤–æ–¥–∏–º –æ—Ü–µ–Ω–∫—É –≥–æ—Å—Ç—è –∫ —à–∫–∞–ª–µ /5.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ñ–æ—Ä–º–∞—Ç—ã:
    - "5", "4,5", "4.0"
    - "9.0" (—Å—á–∏—Ç–∞–µ–º —á—Ç–æ —ç—Ç–æ /10 -> –¥–µ–ª–∏–º –Ω–∞ 2)
    - "80" (—Å—á–∏—Ç–∞–µ–º –∫–∞–∫ % -> –¥–µ–ª–∏–º –Ω–∞ 20)
    - –ø—É—Å—Ç–æ / –º—É—Å–æ—Ä -> NaN
    """
    if val is None:
        return np.nan
    s = str(val).strip().replace(",", ".")
    if s in ("", "-", "‚Äì", "‚Äî"):
        return np.nan
    m = re.search(r"(-?\d+(?:\.\d+)?)", s)
    if not m:
        return np.nan
    v = float(m.group(1))

    # –µ—Å–ª–∏ –≤—ã–≥–ª—è–¥–∏—Ç –∫–∞–∫ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∞—è 1..5
    if 0 <= v <= 5:
        return v
    # —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è 0..10 ‚Üí –¥–µ–ª–∏–º –Ω–∞ 2
    if 0 <= v <= 10:
        return v / 2.0
    # –∏–Ω–æ–≥–¥–∞ % –∏–ª–∏ 0..100 ‚Üí –¥–µ–ª–∏–º –Ω–∞ 20 (100 => 5.0)
    if 0 <= v <= 100:
        return v / 20.0

    return np.nan


# ===== –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã –∞–Ω–∫–µ—Ç—ã =====
def _parse_date_any(x) -> Optional[date]:
    try:
        d = pd.to_datetime(x, errors="coerce", dayfirst=True)
        if pd.isna(d):
            return None
        return d.date()
    except Exception:
        return None


# ===== –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ "—Å—ã—Ä–æ–≥–æ" Excel-–ª–∏—Å—Ç–∞ —Å –∞–Ω–∫–µ—Ç–∞–º–∏ –≤ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º =====
def normalize_surveys_file(df_raw: pd.DataFrame) -> pd.DataFrame:
    """
    –í—Ö–æ–¥: df_raw = DataFrame –∏–∑ XLSX (–ª–∏—Å—Ç —Å –∞–Ω–∫–µ—Ç–∞–º–∏).
    –í—ã—Ö–æ–¥: DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏:
      survey_id (—Å—Ç—Ä–æ–∫–∞ —Ö—ç—à–∞)
      date (YYYY-MM-DD)
      week_key (YYYY-W##, ISO –Ω–µ–¥–µ–ª—è)
      overall5, fo_checkin5, clean_checkin5, room_comfort5,
      fo_stay5, its_service5, hsk_stay5, breakfast5,
      atmosphere5, location5, value5, would_return5,
      nps5,
      comment
    –í—Å–µ –æ—Ü–µ–Ω–∫–∏ –≤ —à–∫–∞–ª–µ /5 (float). nps5 ‚Äî —à–∫–∞–ª–∞ 1‚Äì5 (–∫–∞–∫ –¥–∞–ª –≥–æ—Å—Ç—å).
    """

    df = df_raw.copy()

    # 1) —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏
    colmap: Dict[str, Optional[str]] = {}
    for key, alias_list in COLUMN_ALIASES.items():
        hit = _find_col(df, alias_list)
        colmap[key] = hit  # –º–æ–∂–µ—Ç –±—ã—Ç—å None, –µ—Å–ª–∏ –≤ –≤—ã–≥—Ä—É–∑–∫–µ —Ç–∞–∫–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞ –Ω–µ—Ç

    # 2) –¥–æ—Å—Ç–∞–µ–º –¥–∞—Ç—É –∞–Ω–∫–µ—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ)
    date_col = colmap.get("survey_date")
    if not date_col:
        # fallback —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –∏—â–µ–º –∫–æ–ª–æ–Ω–∫—É, –≥–¥–µ –∫–∞–∫ –º–∏–Ω–∏–º—É–º –ø–æ–ª–æ–≤–∏–Ω–∞ –∑–Ω–∞—á–µ–Ω–∏–π –ø–∞—Ä—Å–∏—Ç—Å—è –∫–∞–∫ –¥–∞—Ç–∞
        best_name = None
        best_hits = 0
        n = len(df)
        for c in df.columns:
            parsed = pd.to_datetime(df[c], errors="coerce", dayfirst=True)
            hits = int(parsed.notna().sum())
            if hits > best_hits and hits >= max(3, int(0.5 * n)):
                best_name = c
                best_hits = hits
        if best_name:
            date_col = best_name
        else:
            raise RuntimeError("–ù–µ –Ω–∞—à–ª–∏ –∫–æ–ª–æ–Ω–∫—É —Å –¥–∞—Ç–æ–π –∞–Ω–∫–µ—Ç–∏—Ä–æ–≤–∞–Ω–∏—è üò¨")

    out = pd.DataFrame()
    out["date"] = df[date_col].map(_parse_date_any)

    # 3) –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π (–æ—Å—Ç–∞–≤–∏–º –∫–∞–∫ –µ—Å—Ç—å)
    if colmap.get("comment"):
        out["comment"] = df[colmap["comment"]].astype(str)
    else:
        out["comment"] = ""

    # 4) —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø–æ–ª—è –≥–æ—Å—Ç—è (–Ω—É–∂–Ω—ã —Ç–æ–ª—å–∫–æ –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏)
    tmp_fio     = df[colmap["fio"]].astype(str)      if colmap.get("fio")      else ""
    tmp_booking = df[colmap["booking"]].astype(str)  if colmap.get("booking")  else ""
    tmp_phone   = df[colmap["phone"]].astype(str)    if colmap.get("phone")    else ""
    tmp_email   = df[colmap["email"]].astype(str)    if colmap.get("email")    else ""

    # 5) –æ—Ü–µ–Ω–∫–∏ ‚Üí —à–∫–∞–ª–∞ /5
    def grab_to5(key: str):
        if not colmap.get(key):
            return np.nan
        return df[colmap[key]].map(_to_5_scale)

    out["overall5"]       = grab_to5("overall")
    out["fo_checkin5"]    = grab_to5("fo_checkin")
    out["clean_checkin5"] = grab_to5("clean_checkin")
    out["room_comfort5"]  = grab_to5("room_comfort")
    out["fo_stay5"]       = grab_to5("fo_stay")
    out["its_service5"]   = grab_to5("its_service")
    out["hsk_stay5"]      = grab_to5("hsk_stay")
    out["breakfast5"]     = grab_to5("breakfast")
    out["atmosphere5"]    = grab_to5("atmosphere")
    out["location5"]      = grab_to5("location")
    out["value5"]         = grab_to5("value")
    out["would_return5"]  = grab_to5("would_return")
    out["nps5"]           = grab_to5("nps5")  # —ç—Ç–∞ –∫–æ–ª–æ–Ω–∫–∞ —É–∂–µ 1..5 –æ—Ç –≥–æ—Å—Ç—è

    # 6) –Ω–µ–¥–µ–ª—è (ISO-–Ω–µ–¥–µ–ª—è) –¥–ª—è –±—ã—Å—Ç—Ä–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ—Ç–æ–º
    def _week_key(d: Optional[date]) -> str:
        if d is None or pd.isna(d):
            return ""
        iso = d.isocalendar()  # (year, week, weekday)
        return f"{iso.year}-W{iso.week:02d}"

    out["week_key"] = out["date"].map(_week_key)

    # 7) –¥–µ–ª–∞–µ–º —Å—Ç–∞–±–∏–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å—Ç—Ä–æ–∫–∏ –∞–Ω–∫–µ—Ç—ã (–¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞ –≤ Google Sheet)
    # –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞—Ç—É + –±—Ä–æ–Ω—å + —Ç–µ–ª–µ—Ñ–æ–Ω + email + –æ–±—â–∏–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π (—ç—Ç–æ –ø–æ—á—Ç–∏ –Ω–∞–≤–µ—Ä–Ω—è–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ)
    def _mk_id(row):
        raw_id = "|".join([
            str(row.get("date", "")),
            str(row.get("comment", "")).strip(),
            # –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –ø–æ–ª—è —Ç–æ–∂–µ —É—á–∞—Å—Ç–≤—É—é—Ç –≤ —Ö—ç—à–µ, –Ω–æ –º—ã –∏—Ö –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ç–∞–±–ª–∏—Ü—É –≤ –æ—Ç–∫—Ä—ã—Ç–æ–º –≤–∏–¥–µ
            # —á—Ç–æ–±—ã –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –∞–Ω–∫–µ—Ç—ã –Ω–µ –≥—Ä—É–∑–∏–ª–∏—Å—å –¥–≤–∞–∂–¥—ã:
            str(tmp_fio[row.name]),
            str(tmp_booking[row.name]),
            str(tmp_phone[row.name]),
            str(tmp_email[row.name]),
        ])
        return hashlib.sha1(raw_id.encode("utf-8")).hexdigest()

    out["survey_id"] = out.apply(_mk_id, axis=1)

    # 8) –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ –¥–∞—Ç—ã (—ç—Ç–æ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –≤—ã–≥—Ä—É–∑–∫–∏)
    out = out[ out["date"].notna() ].reset_index(drop=True)

    # —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ –∫–æ–ª–æ–Ω–æ–∫
    final_cols = [
        "survey_id", "date", "week_key",
        "overall5",
        "fo_checkin5", "clean_checkin5", "room_comfort5",
        "fo_stay5", "its_service5", "hsk_stay5", "breakfast5",
        "atmosphere5", "location5", "value5", "would_return5",
        "nps5",
        "comment",
    ]
    return out[final_cols]
